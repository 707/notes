// [NOT-46] AI Harness - Main Entry Point
// Provides a unified interface for AI interactions with pluggable providers
// This architecture allows easy switching between providers (OpenRouter, Gemini Nano, etc.)

// [NOT-51] Smart Fallback Chain - Auto-generated from OpenRouter API
// The model chain is now loaded from free-models.js (generated by scripts/fetch-free-models.js)
// To update models: run `node scripts/fetch-free-models.js`
// Models are sorted by context window size (largest first) and include only free text-only models
// The FREE_MODEL_CHAIN constant is provided by window.FREE_MODEL_CHAIN (loaded via script tag)

/**
 * AI Harness - Manages AI provider selection and message routing
 * Implements a plugin architecture for multiple AI providers
 */
class AIHarness {
  constructor() {
    this.currentProvider = null;
    this.availableProviders = {
      openrouter: null // Will be initialized lazily
    };
  }

  /**
   * Initialize the harness with a specific provider
   * @param {string} providerName - Provider name ("openrouter", "gemini", etc.)
   * @returns {Promise<boolean>} - Returns true if initialization successful
   */
  async initialize(providerName = 'openrouter') {
    try {
      console.log(`üîå [NOT-46] Initializing AI Harness with provider: ${providerName}`);

      // Initialize OpenRouter provider
      if (providerName === 'openrouter') {
        if (!this.availableProviders.openrouter) {
          // Check if OpenRouterProvider is loaded
          if (typeof OpenRouterProvider === 'undefined') {
            throw new Error('OpenRouterProvider not loaded');
          }
          this.availableProviders.openrouter = new OpenRouterProvider();
        }

        const hasKey = await this.availableProviders.openrouter.initialize();
        if (!hasKey) {
          console.warn('‚ö†Ô∏è  [NOT-46] OpenRouter API key not configured');
          return false;
        }

        this.currentProvider = this.availableProviders.openrouter;
        console.log('‚úÖ [NOT-46] OpenRouter provider initialized');
        return true;
      }

      throw new Error(`Unknown provider: ${providerName}`);
    } catch (error) {
      console.error('‚ùå [NOT-46] Failed to initialize AI Harness:', error);
      return false;
    }
  }

  /**
   * Send a message and receive streaming response
   * [NOT-51] Now implements smart fallback chain when modelId is 'auto'
   * @param {string} text - User message text
   * @param {Object} context - Optional context (previous messages, model, etc.)
   * @param {Function} onChunk - Callback for streaming response chunks
   * @param {Function} onComplete - Callback when response is complete
   * @param {Function} onError - Callback on error
   * @returns {Promise<void>}
   */
  async sendMessage(text, context = {}, onChunk, onComplete, onError) {
    // Ensure provider is initialized
    if (!this.currentProvider) {
      const initialized = await this.initialize('openrouter');
      if (!initialized) {
        onError(new Error('Failed to initialize AI provider. Please check your API key in Settings.'));
        return;
      }
    }

    try {
      // Build messages array from context
      const messages = context.messages || [];

      // Add user message
      messages.push({
        role: 'user',
        content: text
      });

      // Get model ID from context or use default
      const requestedModelId = context.modelId || 'auto';

      // [NOT-51] Determine which models to try based on user preference
      let modelsToTry = [];
      if (requestedModelId === 'auto') {
        // Use the smart fallback chain
        modelsToTry = FREE_MODEL_CHAIN.map(m => m.id);
        console.log('üéØ [NOT-51] Using smart fallback chain with', modelsToTry.length, 'models');
      } else {
        // User selected a specific model - only try that one
        modelsToTry = [requestedModelId];
      }

      // [NOT-51] Try each model in the chain until one succeeds
      let lastError = null;
      let streamStarted = false;

      for (let i = 0; i < modelsToTry.length; i++) {
        const modelId = modelsToTry[i];
        const isLastModel = i === modelsToTry.length - 1;

        console.log(`üí¨ [NOT-51] Attempting model ${i + 1}/${modelsToTry.length}: ${modelId}`);

        try {
          // Wrap callback-based provider API in a Promise
          await new Promise((resolve, reject) => {
            // Track if streaming has started (first chunk received)
            let hasReceivedFirstChunk = false;

            this.currentProvider.sendMessage(
              messages,
              modelId,
              // onChunk wrapper
              (chunk) => {
                if (!hasReceivedFirstChunk) {
                  hasReceivedFirstChunk = true;
                  streamStarted = true;
                  console.log(`‚úÖ [NOT-51] Stream started successfully with ${modelId}`);
                }
                onChunk(chunk);
              },
              // onComplete wrapper
              () => {
                console.log(`‚úÖ [NOT-51] Message completed successfully with ${modelId}`);
                onComplete();
                resolve();
              },
              // onError wrapper
              (error) => {
                // If stream already started, don't retry (partial response received)
                if (hasReceivedFirstChunk) {
                  console.error(`‚ùå [NOT-51] Stream failed mid-response with ${modelId}:`, error);
                  onError(error);
                  resolve(); // Don't retry - user already saw partial response
                } else {
                  // Stream never started - this is a retriable error
                  console.warn(`‚ö†Ô∏è  [NOT-51] Model ${modelId} failed before streaming:`, error.message);
                  lastError = error;
                  reject(error);
                }
              }
            );
          });

          // If we get here, the message succeeded
          return;

        } catch (error) {
          lastError = error;

          // If this is the last model, propagate the error
          if (isLastModel) {
            console.error(`‚ùå [NOT-51] All models exhausted. Last error:`, error);
            onError(new Error(`All models failed. Last error: ${error.message}`));
            return;
          }

          // Otherwise, log and continue to next model
          console.log(`üîÑ [NOT-51] Retrying with next model in chain...`);
        }
      }

    } catch (error) {
      console.error('‚ùå [NOT-51] Error in sendMessage:', error);
      onError(error);
    }
  }

  /**
   * Get list of available models for current provider
   * [NOT-51] Returns the full fallback chain plus an 'auto' option
   * @returns {Array} - Array of model objects with id, name, and description
   */
  getAvailableModels() {
    // [NOT-51] Return 'auto' option first (recommended), then all individual models
    return [
      {
        id: 'auto',
        name: 'Smart Auto (Recommended)',
        description: 'Intelligent fallback chain prioritized by context window'
      },
      ...FREE_MODEL_CHAIN
    ];
  }

  /**
   * Test if current provider is configured correctly
   * @returns {Promise<boolean>} - Returns true if provider is ready
   */
  async testProvider() {
    if (!this.currentProvider) {
      return false;
    }

    try {
      if (this.currentProvider.testApiKey) {
        return await this.currentProvider.testApiKey();
      }
      return true;
    } catch (error) {
      console.error('‚ùå [NOT-46] Provider test failed:', error);
      return false;
    }
  }
}

// Export global instance
window.aiHarness = new AIHarness();
console.log('‚úÖ [NOT-46] AI Harness loaded');
